{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f531b-f1de-499d-92d2-65904fc0ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d8be43-5ec2-49b3-acd1-0cb06bb56666",
   "metadata": {
    "id": "j7ch3dDfYgsj"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datasets\n",
    "import re\n",
    "import gc \n",
    "import argparse\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "import evaluate\n",
    "from statistics import mean\n",
    "from evaluate import load\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "pd.set_option('display.max_colwidth', None)  # None means unlimited width\n",
    "# Load the BERTScore evaluation metric\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "MAX_SEQ_LENGTH = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "DTYPE = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "LOAD_IN_4BIT = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "DATA_PATH = \"/cluster/home/pgoyal/main/test/cot_test.csv\"\n",
    "OUTPUT_PATH = \"/cluster/project/sachan/piyushi/predictions_COT\"\n",
    "checkpoint_path = \"/cluster/project/sachan/piyushi/merged_models_COT/checkpoint-360\"\n",
    "SEED = 42\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb555e9-dbc4-4802-9efb-1310d4e79dd4",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daea072-be64-4009-b890-db7a4d1916d2",
   "metadata": {
    "id": "K8yYb5xQnP-M"
   },
   "outputs": [],
   "source": [
    "def prepare_test_data(path=DATA_PATH, remove_test_duplicates=True):\n",
    "    # Load test data\n",
    "    df_test = pd.read_csv(path)\n",
    "    df_test.rename(columns={'answer':'response'}, inplace=True)\n",
    "    \n",
    "    # Sort test set by 'id'\n",
    "    df_test = df_test.sort_values(by=['id'])\n",
    "    \n",
    "    # Remove Duplicates for Test DF if necessary\n",
    "    if remove_test_duplicates:\n",
    "        print(\"Removing Test Duplicates\")\n",
    "        df_test = df_test.drop_duplicates(subset=['id'])\n",
    "        df_test.reset_index(drop=True, inplace=True)\n",
    "        print(df_test.shape)\n",
    "        print(df_test.head())\n",
    "    \n",
    "    # Convert to Dataset\n",
    "    dataset_test = datasets.Dataset.from_pandas(df_test[['id','question','response']].copy())\n",
    "    \n",
    "    # Create DatasetDict with only 'test'\n",
    "    ds = datasets.DatasetDict({\"test\": dataset_test})\n",
    "    \n",
    "    print(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1532e64-d030-4ff6-a211-4152ea878a73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1915e4d-08fb-4d37-86ea-e2c321200b4f",
    "outputId": "977926ec-9bd8-417b-b9bb-4457b390229d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = prepare_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd8997c-3b18-485f-be47-16774ceb8f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cd800-4bd3-4dc4-821b-ad9399cc20fe",
   "metadata": {},
   "source": [
    "### VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37e813-15f8-48f7-8c6b-c83e88965983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07fcf1f-baa0-422f-971c-da1d5fb91ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ans(step):\n",
    "    pattern = r'<<.*?=(\\d+)>>'\n",
    "    match = re.search(pattern, step)\n",
    "    \n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    \n",
    "    # Fallback to original pattern\n",
    "    number_pattern = r'[$]?[-+]?\\d+(?:\\.\\d+)?(?:,\\d+)*[$]?'\n",
    "    matches = re.findall(number_pattern, step)\n",
    "    \n",
    "    if matches:\n",
    "        cleaned_value = float(matches[-1].replace(\",\", \"\").replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"$\", \"\").replace(\"x\", \"\"))\n",
    "        return cleaned_value\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad0be7-b1cd-45fc-a611-7b58a83dd024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_steps(text):\n",
    "    # Split the text by the newline character\n",
    "    steps = text.split('\\n')\n",
    "    \n",
    "    # Remove any empty strings and filter out steps that start with '####'\n",
    "    steps = [step for step in steps if step.strip() and not step.strip().startswith('####')]\n",
    "    \n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d4099-45ee-4ad7-abec-9b34fc855054",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "true_test_step = []\n",
    "first_step_ans = []\n",
    "answers = []\n",
    "\n",
    "for item in ds['test']:\n",
    "    questions.append(item['question'])\n",
    "    steps = split_into_steps(item['response'])\n",
    "    true_test_step.append(steps[0])\n",
    "    first_step_ans.append(extract_ans(steps[0]))\n",
    "    answers.append(item['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9f6e7-f38a-432d-b46a-02f524d6ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\n",
    "    'question': questions,\n",
    "    # 'answer': answers\n",
    "    'correct_first_step': true_test_step,\n",
    "    'gt_ans': first_step_ans\n",
    "})\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad704f2-44a1-439a-b68c-382f98b5f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompts_batch(checkpoint_path, questions, first_steps, flag):\n",
    "    \"\"\"\n",
    "    Formats a batch of questions and first steps using ChatML template.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the checkpoint of the model.\n",
    "        questions (list of str): List of math word problems.\n",
    "        first_steps (list of str): List of first steps for each question.\n",
    "\n",
    "    Returns:\n",
    "        list of torch.Tensor: List of formatted and tokenized prompts ready for input.\n",
    "    \"\"\"\n",
    "    tokenizer = get_chat_template(\n",
    "        AutoTokenizer.from_pretrained(checkpoint_path),  # Adjust this function to your needs\n",
    "        chat_template=\"chatml\",\n",
    "        mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gemma\"},\n",
    "        map_eos_token=True\n",
    "    )\n",
    "\n",
    "    formatted_questions = []\n",
    "\n",
    "    for question, first_step in zip(questions, first_steps):\n",
    "        if flag:\n",
    "            formatted_question = tokenizer.apply_chat_template(\n",
    "                [{\"from\": \"human\", \"value\": f\"### Instruction:\\nCalculate only the first step for the following Math Word Problem\\n\\n### Input:\\n{question}\"}],\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(\"cuda\")\n",
    "        else:\n",
    "            formatted_question = tokenizer.apply_chat_template(\n",
    "                [\n",
    "                    {\"from\": \"human\", \"value\": f\"### Instruction:\\nCalculate only the first step for the following Math Word Problem\\n\\n### Input:\\n{question}\"},\n",
    "                    {\"from\": \"gemma\", \"value\": f\"{first_step}\"},\n",
    "                    {\"from\": \"human\", \"value\": f\"### Instruction:\\nContinue generating the entire answer from the next step\\n\\n\"}\n",
    "                ],\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(\"cuda\")\n",
    "        formatted_questions.append(formatted_question)\n",
    "    \n",
    "    return formatted_questions, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4898affe-3e7c-4390-b271-8156626cfb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_steps_batch(checkpoint_path, max_seq_len, formatted_questions, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates steps for a batch of math word problems using a pre-trained language model.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the model checkpoint.\n",
    "        max_seq_len (int): Maximum sequence length for token generation.\n",
    "        formatted_questions (list of torch.Tensor): List of tokenized prompts for the batch.\n",
    "        tokenizer (AutoTokenizer): The tokenizer used for decoding.\n",
    "\n",
    "    Returns:\n",
    "        list of str: List of generated texts for the batch.\n",
    "    \"\"\"\n",
    "    # Load the model and sampling parameters\n",
    "    llm = LLM(model=checkpoint_path)\n",
    "    sampling_params = SamplingParams(temperature=0, max_tokens=max_seq_len, stop=[\"### Instruction\", \"### Input\"])\n",
    "    \n",
    "    predictions = []\n",
    "    decoded_questions = []\n",
    "    \n",
    "    for formatted_question in formatted_questions:\n",
    "        token_ids = formatted_question.squeeze().tolist()\n",
    "        decoded_question = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "        decoded_questions.append(decoded_question)\n",
    "    # Generate outputs for each question\n",
    "    outputs = llm.generate(decoded_question, sampling_params)\n",
    "    \n",
    "    for output in outputs:\n",
    "        generated_text = output.outputs[0].text\n",
    "        predictions.append(generated_text)\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    torch.cuda.empty_cache()\n",
    "    del llm\n",
    "    del sampling_params\n",
    "    gc.collect()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095989c-ad5e-4050-b90c-87410c537231",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flag = True\n",
    "\n",
    "first_steps = [\"\"] * len(questions)\n",
    "first_step_prompts, tokenizer = format_prompts_batch(checkpoint_path, questions, first_steps, flag)\n",
    "first_step_predictions = generate_steps_batch(checkpoint_path, MAX_SEQ_LENGTH, first_step_prompts, tokenizer)\n",
    "df_test['pred_first_step'] = first_step_predictions\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb213ee4-5b24-4ac5-b441-29717ced20c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_rows = []\n",
    "\n",
    "for idx, first_step in enumerate(df_test['pred_first_step']):\n",
    "    ans = extract_ans(first_step)\n",
    "    ans_right = df_test.loc[idx, 'gt_ans']\n",
    "    if ans == ans_right:\n",
    "        true_rows.append({\n",
    "            'question': df_test.loc[idx, 'question'],\n",
    "            'true_answer': ds['test'][idx]['response'],\n",
    "            'pred_first_step': first_step,\n",
    "            'final_true_answer': ds['test'][idx]['response'].split(\"####\")[1].strip()\n",
    "        })\n",
    "\n",
    "# Create a new DataFrame with only the rows where the condition was true\n",
    "df_correct = pd.DataFrame(true_rows)\n",
    "df_correct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef687ce-cd1f-48c8-b8ba-7ca447ca3875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flag = False\n",
    "remaining_step_prompts, tokenizer = format_prompts_batch(checkpoint_path, df_correct['question'].tolist(), df_correct['pred_first_step'].tolist(), flag)\n",
    "remaining_step_predictions = generate_steps_batch(checkpoint_path, MAX_SEQ_LENGTH, remaining_step_prompts, tokenizer)\n",
    "df_correct['remaining_steps'] = remaining_step_predictions\n",
    "df_correct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb662cf2-bac8-4ac0-97fd-97aa16b37405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_ans(step):\n",
    "    # Find all matches of the pattern <<...=value>> and extract the last one\n",
    "    pattern = r'<<.*?=(\\d+(?:\\.\\d+)?)>>'\n",
    "    matches = re.findall(pattern, step)\n",
    "    \n",
    "    if matches:\n",
    "        # Return the last match (the answer from the last step)\n",
    "        return float(matches[-1])\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c5d4c8-e7bc-4612-b5c8-af076193e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "pattern = r'## Final Answer: (\\d+(?:\\.\\d+)?)'\n",
    "for index, row in df_correct.iterrows(): \n",
    "    # pred_final = extract_ans(row['remaining_steps_answer']) \n",
    "    steps = row['remaining_steps']\n",
    "    if \"## Final Answer: \" in steps:\n",
    "        total += 1\n",
    "        pred_final = steps.split(\"## Final Answer:\")[1].strip()\n",
    "        gt_final = df_correct.loc[index, 'final_true_answer']\n",
    "        # print(gt_final)\n",
    "        if pred_final == gt_final: \n",
    "            count += 1\n",
    "print(total, count / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6155cd00-2c92-4796-9af9-0bf9fe43f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_pickle('dftest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b9b51-7add-4599-a786-042298075158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle('dftest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c319ba-97a6-445d-a541-3da682bcd864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb67b0-ad0e-41f0-8124-2fa0265269a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "tot=0\n",
    "for idx, first_step in enumerate(df_test['pred_first_step']):\n",
    "    tot+=1\n",
    "    ans = extract_ans(first_step)\n",
    "    ans_right = df_test.loc[idx, 'gt_ans']\n",
    "    if ans == ans_right:\n",
    "        cnt += 1\n",
    "print(tot, cnt/tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea0770-19d2-41c0-b551-1666eb974f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_and_final(index, text):\n",
    "    first=final=\"\"\n",
    "    first = text.split(\"\\n\")[0].strip()\n",
    "    if \"####\" in text:\n",
    "        final = text.split(\"####\")[1].strip()\n",
    "    else: print(index, text)\n",
    "    return first, final\n",
    "\n",
    "count_first = 0\n",
    "tot_first = 0\n",
    "count_final = 0\n",
    "tot_final = 0\n",
    "\n",
    "for index, row in df_test.iterrows(): \n",
    "    ans = row['pred_first_step']\n",
    "    if \"### Answer:\\n\" in ans:\n",
    "        tot_first += 1\n",
    "        only_ans = ans.split(\"### Answer:\\n\")[1].strip()\n",
    "        pred_first, pred_final = first_and_final(index, only_ans)\n",
    "        gt_ans = row['answer']\n",
    "        gt_first, gt_final = first_and_final(index, gt_ans)\n",
    "        pred_first_ans = get_final_ans(pred_first)\n",
    "        gt_first_ans = get_final_ans(gt_first)\n",
    "        if gt_first_ans == pred_first_ans:\n",
    "            count_first += 1\n",
    "            if pred_final != \"\":\n",
    "                tot_final += 1\n",
    "                if gt_final == pred_final:\n",
    "                    count_final += 1\n",
    "        # gt_ans = row['answer']\n",
    "    # gt_first, gt_final = first_and_final(index, gt_ans)\n",
    "    # pred_first_ans = get_final_ans(pred_first)\n",
    "    # gt_first_ans = get_final_ans(gt_first)\n",
    "    # if gt_first_ans == pred_first_ans:\n",
    "    #     count_first += 1\n",
    "    # if pred_final != \"\" and gt_final == pred_final:\n",
    "    #     count_final += 1\n",
    "print(tot_first, tot_final)\n",
    "print(f\"First answer accuracy = {count_first/tot_first}\")\n",
    "print(f\"Final answer accuracy = {count_final/tot_final}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sem_project",
   "language": "python",
   "name": "sem_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
