{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75b4201-65af-42ec-bebb-bc741406bf61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d75b4201-65af-42ec-bebb-bc741406bf61",
    "outputId": "9dc81871-d59f-4f4b-8f42-dfc0ea120431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 20 01:22:29 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:E1:00.0 Off |                  Off |\n",
      "| 58%   50C    P5              53W / 450W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc967f6-5907-48d6-8fd7-76ae42cc0e12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3326731-1cc8-4596-9049-3a7a4d9b3c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91bc8954-6537-4d1f-b3cd-0f29f91aaa9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip3 install \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "j7ch3dDfYgsj",
   "metadata": {
    "id": "j7ch3dDfYgsj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datasets\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import EvalPrediction\n",
    "import re\n",
    "import json\n",
    "import gc \n",
    "import argparse\n",
    "import os\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import wandb\n",
    "pd.set_option('display.max_colwidth', None)  # None means unlimited width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ffc3e8-0fae-44b9-a244-4e1c38acd85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "DTYPE = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "LOAD_IN_4BIT = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "DATA_PATH = \"/cluster/home/pgoyal/main/test/COT/cot_train.csv\"\n",
    "# MODEL_NAME = \"unsloth/gemma-2b-it\"\n",
    "# output_dir = \"/cluster/project/sachan/piyushi/merged_models_COT\"\n",
    "# checkpoint_dir = \"/cluster/project/sachan/piyushi/final_predictions_COT/gemma_2b\"\n",
    "MODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct\"\n",
    "OUTPUT_PATH = \"/cluster/project/sachan/piyushi/final_predictions_COT/qwen_7\"\n",
    "merged_dir = \"/cluster/project/sachan/piyushi/merged_models_COT/qwen_7\"\n",
    "SEED = 42\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76f56ec6-99a7-42f0-832a-a97b27413cc2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382,
     "referenced_widgets": [
      "859111dfc6484a93b682427fe49c21f6",
      "84f60cec9dac4ea6bd2b13470ec4b097",
      "0351a104df874edf885ea696a63cdcea",
      "bacf7efd31fe495fbf14009a9a36c6d9",
      "3f2c28540c5d4ff7bfca8d53cf33599a",
      "fe9d546e76d84d3f977ecbf14d1cbe31",
      "0f8fb3ec0d9d4edc93669913dbc75c54",
      "c8a3971adbe3442c8640af9684a2509d",
      "6eeb4359964a45c98fdeb8dd954d6342",
      "b60f1a94d72f402eb3f0b5dfead1532a",
      "e8bdfb6d1b3a429fa83655180ba26101",
      "1214859312a74a4d952c8aa66b51b502",
      "2973dfba4b514cf98d8be3d3835f1d58",
      "060421c7756243ea8c44aad8b4dd8a8d",
      "988e6bc94d574b63b936d8dbbd0318c4",
      "3ffc2b15d085400bb70cf2e9dd2d3138",
      "202478512209441da0272d919c94a3c6",
      "2197b3204ac14659855c233b6316ee4d",
      "0328f7cd4dd04317aac32175e82920dd",
      "8ccd0348be8e47e996a2967916fc1650",
      "8945fd3df7d6403ea564ff90c531ff5f",
      "dc3127b9b6074788beec6f8a73fd20ac",
      "eba551f4cef645e4a5c8cdf6607efaf9",
      "01101eff78de4cb99c85617a796d133d",
      "3f90dd77826c49a8991c06177d4d66df",
      "b33ce1ebd82341a08ee25d7b78a5e531",
      "48aec7b783b3433d87de9354cd00e9df",
      "018405878b844f9ca12262b4fa22718a",
      "6c5e9ec3149f4082b7d3ddefa6b54538",
      "21374b31b5d04e909b139ad521442bb3",
      "ef59e1e5621f4df89b525658af407ac9",
      "dabab48f1de44f1ebb065f0cccd1f906",
      "0c4489c51c904bd888d31e2c7ef1b9ff",
      "fc8596d3cca248ec857faee5ae4c971d",
      "5b4b2a7035834bcda88c96f1498fbaf2",
      "847b226bed834b75b337a17ba40356dd",
      "2ee292c69c8445038eaadee2062618f6",
      "99678ccd77ba401fa3fd76bea9ba115e",
      "77fe7f75b37744d6a29de42b5bc2a27d",
      "07fd029518c345539da2c34f4e9c2e5a",
      "f2f61117276b4b619e1bbd77ccb6e8f0",
      "627a11ddc1074bf6b33e3c0d40a1b411",
      "77831f59dc224396a0b936491c0a5bd1",
      "8c411d40c82f412683fd888f827cd65b",
      "f9560828026d4dcc925825b047a775f0",
      "35f63e42ad154969a55e9a7dafb5131a",
      "3a3863b1d33641d6b2a7e4acb3f93e83",
      "15248b68ac7442c5a842ee6f4c7c2490",
      "3bf424767ceb4eaeaac47fd30c8c648c",
      "f66534d7240846bca4dcd12695f609d2",
      "1c9c8ea3c64f479eb98e49fe7bed0295",
      "bd681496164a40d18762a93901460cef",
      "91387aeab12f4dc6a7089f8338b65973",
      "5fb6b53d367e497ab781e27423f99ba6",
      "a67f7dd2ecf546fc90c84a75768436a0",
      "0b04f611162b4785a7736cda181bc002",
      "a9105b486b8d496586f7d7fc7b64e0b1",
      "cb1d26f2297e4848b1c4728fb8edeba6",
      "0817ec35e7cd46bc98111742c684023c",
      "b4e66b975e4a40e9aadfaa8830955a56",
      "684835aef4ca4f54bd4b311ef16e84c7",
      "a41ccb7d8a3440ad8b71ab63b4a74270",
      "f5cc63164ed5451f9f7a9fdd2de576ef",
      "c00d9ac1cda54f33b3a87b4debc50cb6",
      "31ce58be2e73465191c069852f3d6b3f",
      "c6712af364e24893acaa8921edf434f7",
      "84285fc67dd642bea2ddb25fe29cde80",
      "72673d88c5e3474d83adf64626bf0ae8",
      "0e9b27164e1445c6bf2d098531a7b537",
      "6e410518d60c40ceacae77e741f4da2e",
      "eb2c8f251df840c5ba110c2dbc4ebb8a",
      "fb3eff46f77f44c8bb90fbb921f1c09e",
      "f3e45d320a654facaaeca1b640e7be68",
      "a7a750c867574d0a98e492f7a4999026",
      "30e61f4b084f46d4bd642d54106f1dda",
      "2227ec266d644df4b5cd60163d196745",
      "abd831de9f434ba5a73167b44d2dbe4c"
     ]
    },
    "id": "76f56ec6-99a7-42f0-832a-a97b27413cc2",
    "outputId": "c0b3fee5-d4d8-4ba3-9849-db809417c609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Qwen2 patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.65 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bef2b756cc94385ac9778bcc5bddc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_NAME,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = DTYPE,\n",
    "    load_in_4bit = LOAD_IN_4BIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6175cb5e-e1c3-4d22-bdbc-335007e49ef9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6175cb5e-e1c3-4d22-bdbc-335007e49ef9",
    "outputId": "96512995-ed4e-499a-cd49-b8bda1a3d810"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. No common IDs between train and validation.\\n2. See all IDs first [(id0-sample0, id1-sample0, id2-sample0,..., id1-sample1, id2-sample1, ...)]\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. No common IDs between train and validation.\n",
    "2. See all IDs first [(id0-sample0, id1-sample0, id2-sample0,..., id1-sample1, id2-sample1, ...)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9db0176a-8b18-4d36-9e51-4986ac689947",
   "metadata": {
    "id": "9db0176a-8b18-4d36-9e51-4986ac689947"
   },
   "outputs": [],
   "source": [
    "# Function to arrange dataframe such that the first n rows correspond to first n responses, and so on. \n",
    "# The idea is that the model sees all IDs first, instead of all responses from an ID \n",
    "def arrange_df(df, id_col='id', ques_col='question',response_col='response'):\n",
    "    df = df.sort_values(by=[id_col])\n",
    "    \n",
    "    # Group dataframe by 'id' column\n",
    "    grouped = df.groupby(id_col)\n",
    "    \n",
    "    # Initialize an empty list to store the arranged data\n",
    "    arranged_data = []\n",
    "    \n",
    "    # Get unique IDs and the maximum number of responses for any ID\n",
    "    unique_ids = df[id_col].unique()\n",
    "    max_responses = grouped.size().max()\n",
    "    \n",
    "    # Iterate over the number of responses (max_responses)\n",
    "    for i in range(max_responses):\n",
    "        # Iterate over unique IDs\n",
    "        for id_ in unique_ids:\n",
    "            # Get all responses and questions for the current ID\n",
    "            responses = grouped.get_group(id_)[response_col].tolist()\n",
    "            questions = grouped.get_group(id_)[ques_col].tolist()\n",
    "            # Append the ID, question, and response if available, else append None\n",
    "            if i < len(responses):\n",
    "                arranged_data.append({'id': id_, 'question': questions[i], 'response': responses[i]})\n",
    "            else:\n",
    "                arranged_data.append({'id': id_, 'question': None, 'response': None})\n",
    "    \n",
    "    # Create a new DataFrame from the arranged data\n",
    "    arranged_df = pd.DataFrame(arranged_data)\n",
    "    arranged_df = arranged_df.dropna(subset=['response'])\n",
    "    arranged_df.reset_index(drop=True, inplace=True)\n",
    "    print(arranged_df.head(10))\n",
    "    \n",
    "    assert(arranged_df.shape[0] == df.shape[0])\n",
    "    return arranged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc5N_JDvnMGO",
   "metadata": {
    "id": "dc5N_JDvnMGO"
   },
   "outputs": [],
   "source": [
    "def split_data(df, ratio=0.8):\n",
    "    \"\"\"\n",
    "    Splits the data into train and validation sets ensuring no common IDs among them.\n",
    "    \"\"\"\n",
    "    ids = list(set(df['id'].tolist()))\n",
    "    random.shuffle(ids)\n",
    "\n",
    "    ntrain = int(ratio*len(ids))\n",
    "    train_ids = ids[:ntrain]\n",
    "    val_ids = ids[ntrain:]\n",
    "\n",
    "    df_train = df[df['id'].isin(train_ids)].copy()\n",
    "    df_val = df[df['id'].isin(val_ids)].copy()\n",
    "\n",
    "    print(\"Train shape: \", df_train.shape)\n",
    "    print(\"Val shape: \", df_val.shape)\n",
    "    print(\"Data distribution: Train: {:.2f}, Val: {:.2f}\".format(df_train.shape[0]/len(df), df_val.shape[0]/len(df)))\n",
    "\n",
    "    return df_train, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "K8yYb5xQnP-M",
   "metadata": {
    "id": "K8yYb5xQnP-M"
   },
   "outputs": [],
   "source": [
    "def prepare_data(path=DATA_PATH, arrange_train=False, remove_train_duplicates=True, remove_val_duplicates=True, split_ratio=0.8):\n",
    "    df = pd.read_csv(path)\n",
    "    df.rename(columns={'answer':'response'}, inplace=True)\n",
    "    # Get Train and Val DFs\n",
    "    \n",
    "    df_train, df_val = split_data(df, split_ratio)\n",
    "    df_train = df_train.sort_values(by=['id'])\n",
    "    df_val = df_val.sort_values(by=['id'])\n",
    "    \n",
    "    if remove_train_duplicates:\n",
    "        print(\"Removing Train Duplicates\")\n",
    "        df_train = df_train.drop_duplicates(subset=['id'])\n",
    "        df_train.reset_index(drop=True, inplace=True) \n",
    "        print(df_train.shape)\n",
    "        print(df_train.head())\n",
    "    \n",
    "    # Arrange DF Train\n",
    "    if arrange_train:\n",
    "        df_train = arrange_df(df_train)\n",
    "    \n",
    "    # Remove Duplicates for Validation DF \n",
    "    if remove_val_duplicates:\n",
    "        print(\"Removing Val Duplicates\")\n",
    "        df_val = df_val.drop_duplicates(subset=['id'])\n",
    "        df_val.reset_index(drop=True, inplace=True)\n",
    "        print(df_val.shape)\n",
    "        print(df_val.head())\n",
    "        \n",
    "    # Convert to Dataset \n",
    "    dataset_train = datasets.Dataset.from_pandas(df_train[['id','question','response']].copy())\n",
    "    dataset_val = datasets.Dataset.from_pandas(df_val[['id','question','response']].copy())\n",
    "    \n",
    "    # Dataset Dict\n",
    "    ds = datasets.DatasetDict({\"train\":dataset_train, \"val\":dataset_val})\n",
    "    \n",
    "    print(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "797a4561-9681-4f53-a0ad-89b7c33c727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_steps(text):\n",
    "    # Split the text by the newline character\n",
    "    steps = text.split('\\n')\n",
    "    \n",
    "    # Remove any empty strings and filter out steps that start with '####'\n",
    "    steps = [step for step in steps if step.strip() and not step.strip().startswith('####')]\n",
    "    final_ans = text.split(\"####\")[1].strip()\n",
    "    return steps, final_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a418eee0-d209-4148-9505-27b1e83c9b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n",
      "/cluster/home/pgoyal/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define your ChatML template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gemma\"},  # ShareGPT style\n",
    "    map_eos_token=True,  # Map <|im_end|> to </s>\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"question\"]\n",
    "    outputs = examples[\"response\"]\n",
    "    texts = []\n",
    "\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        steps, final_ans = split_into_steps(output)\n",
    "        remaining_steps = ' '.join(steps[1:])\n",
    "        \n",
    "        # ChatML structure\n",
    "        convo = [\n",
    "            {\"from\": \"human\", \"value\": f\"### Instruction:\\nCalculate only the first step for the following Math Word Problem\\n\\n### Input:\\n{input}\"},\n",
    "            {\"from\": \"gemma\", \"value\": f\"### First Step:\\n{steps[0]}\"},\n",
    "            {\"from\": \"human\", \"value\": f\"### Instruction:\\nContinue generating the entire answer from the next step\\n\\n\"},\n",
    "            {\"from\": \"gemma\", \"value\": f\"### Next steps:\\n{remaining_steps}\" + f\"\\n## Final Answer: \" + f\"{final_ans}\"},\n",
    "        ]\n",
    "        # convo = [\n",
    "        #     {\"from\": \"human\", \"value\": f\"### Instruction:\\nSolve the following Math Word Problem\\n\\n### Input:\\n{input}\"},\n",
    "        #     {\"from\": \"gemma\", \"value\": f\"### Answer:\\n{output}\"}\n",
    "        # ]\n",
    "        text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        texts.append(text)\n",
    "\n",
    "    return {\"text\": texts}\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1915e4d-08fb-4d37-86ea-e2c321200b4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1915e4d-08fb-4d37-86ea-e2c321200b4f",
    "outputId": "977926ec-9bd8-417b-b9bb-4457b390229d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (5978, 3)\n",
      "Val shape:  (1495, 3)\n",
      "Data distribution: Train: 0.80, Val: 0.20\n",
      "Removing Train Duplicates\n",
      "(5978, 3)\n",
      "   id  \\\n",
      "0   1   \n",
      "1   2   \n",
      "2   3   \n",
      "3   6   \n",
      "4   7   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                question  \\\n",
      "0                                                                                                                            Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?   \n",
      "1                                                                                                                                                                      Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?   \n",
      "2                   Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?   \n",
      "3  Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?   \n",
      "4                                                              Albert is wondering how much pizza he can eat in one day. He buys 2 large pizzas and 2 small pizzas. A large pizza has 16 slices and a small pizza has 8 slices. If he eats it all, how many pieces does he eat that day?   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                           response  \n",
      "0                                                                                                                                                                                                                                                                  Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72  \n",
      "1                                                                                                                                                                                                                                                                            Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\\n#### 10  \n",
      "2                                                                                                                                                                                              In the beginning, Betty has only 100 / 2 = $<<100/2=50>>50.\\nBetty's grandparents gave her 15 * 2 = $<<15*2=30>>30.\\nThis means, Betty needs 100 - 50 - 30 - 15 = $<<100-50-30-15=5>>5 more.\\n#### 5  \n",
      "3  There are 80/100 * 10 = <<80/100*10=8>>8 more purple flowers than yellow flowers.\\nSo in Mark's garden, there are 10 + 8 = <<10+8=18>>18 purple flowers.\\nPurple and yellow flowers sum up to 10 + 18 = <<10+18=28>>28 flowers.\\nThat means in Mark's garden there are 25/100 * 28 = <<25/100*28=7>>7 green flowers.\\nSo in total Mark has 28 + 7 = <<28+7=35>>35 plants in his garden.\\n#### 35  \n",
      "4                                                                                                                                                                                                      He eats 32 from the largest pizzas because 2 x 16 = <<2*16=32>>32\\nHe eats 16 from the small pizza because 2 x 8 = <<2*8=16>>16\\nHe eats 48 pieces because 32 + 16 = <<32+16=48>>48\\n#### 48  \n",
      "Removing Val Duplicates\n",
      "(1495, 3)\n",
      "   id  \\\n",
      "0   4   \n",
      "1   5   \n",
      "2  10   \n",
      "3  14   \n",
      "4  18   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                       question  \\\n",
      "0                                                                                                   Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?   \n",
      "1                                                                                                                                                                                                                       James writes a 3-page letter to 2 different friends twice a week.  How many pages does he write a year?   \n",
      "2                                                                                      Tina makes $18.00 an hour.  If she works more than 8 hours per shift, she is eligible for overtime, which is paid by your hourly wage + 1/2 your hourly wage.  If she works 10 hours every day for 5 days, how much money does she make?   \n",
      "3                                    Jasper will serve charcuterie at his dinner party. He buys 2 pounds of cheddar cheese for $10, a pound of cream cheese that cost half the price of the cheddar cheese, and a pack of cold cuts that cost twice the price of the cheddar cheese. How much does he spend on the ingredients?   \n",
      "4  In a truck, there are 26 pink hard hats, 15 green hard hats, and 24 yellow hard hats.  If Carl takes away 4 pink hard hats, and John takes away 6 pink hard hats and twice as many green hard hats as the number of pink hard hats that he removed, then calculate the total number of hard hats that remained in the truck.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   response  \n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Maila read 12 x 2 = <<12*2=24>>24 pages today.\\nSo she was able to read a total of 12 + 24 = <<12+24=36>>36 pages since yesterday.\\nThere are 120 - 36 = <<120-36=84>>84 pages left to be read.\\nSince she wants to read half of the remaining pages tomorrow, then she should read 84/2 = <<84/2=42>>42 pages.\\n#### 42  \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     He writes each friend 3*2=<<3*2=6>>6 pages a week\\nSo he writes 6*2=<<6*2=12>>12 pages every week\\nThat means he writes 12*52=<<12*52=624>>624 pages a year\\n#### 624  \n",
      "2  She works 8 hours a day for $18 per hour so she makes 8*18 = $<<8*18=144.00>>144.00 per 8-hour shift\\nShe works 10 hours a day and anything over 8 hours is eligible for overtime, so she gets 10-8 = <<10-8=2>>2 hours of overtime\\nOvertime is calculated as time and a half so and she makes $18/hour so her overtime pay is 18*.5 = $<<18*.5=9.00>>9.00\\nHer overtime pay is 18+9 = $<<18+9=27.00>>27.00\\nHer base pay is $144.00 per 8-hour shift and she works 5 days and makes 5 * $144 = $<<144*5=720.00>>720.00\\nHer overtime pay is $27.00 per hour and she works 2 hours of overtime per day and makes 27*2 = $<<27*2=54.00>>54.00 in overtime pay\\n2 hours of overtime pay for 5 days means she makes 54*5 = $270.00\\nIn 5 days her base pay is $720.00 and she makes $270.00 in overtime pay so she makes $720 + $270 = $<<720+270=990.00>>990.00\\n#### 990  \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    A pound of cream cheese cost $10 / 2 = $<<10/2=5>>5.\\nA pack of cold cuts cost $10 x 2 = $<<10*2=20>>20.\\nJasper spent $10 + $5 + $20 = $<<10+5+20=35>>35 on the ingredients.\\n#### 35  \n",
      "4                                                                                                                                                                  If there were 26 pink hard hats and Carl took away 4 pink hard hats, the number of pink hard hats that remained is 26-4 = <<26-4=22>>22\\nJohn also took away 6 pink hard hats, leaving 22-6 = <<22-6=16>>16 pink hard hats in the truck.\\nIf John also took twice as many green hard hats as pink hard hats, he took 2*6 = <<6*2=12>>12 green hard hats.\\nThe total number of green hard hats that remained in the truck is 15-12 = <<15-12=3>>3\\nIn the truck, after some are taken, there were 3 green hard hats + 16 pink hard hats = <<3+16=19>>19 hard hats in the truck.\\nAltogether, 19 green and pink hard hats + 24 yellow hards hats = <<19+24=43>>43 hard hats remained in the truck\\n#### 43  \n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'response'],\n",
      "        num_rows: 5978\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['id', 'question', 'response'],\n",
      "        num_rows: 1495\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds = prepare_data(split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6078eaa-f0d1-41a4-96bb-d603453b3f8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "c69696c786e94ea8a8f6de2a88f185a0",
      "3b4630c1066a46eeadf5e40ffd9e6600",
      "ab10146d47fb43e3a1d491bfd87fe294",
      "5d57f55d6ce840d791159cc722a7af39",
      "efd1e04469f54a3cbdce102e6ab784d0",
      "50d629dedeba497a8f61bb502ecbd6a1",
      "665b1eaab4de400685c0f277c84ab47a",
      "c45fef4985e34e2a9151bd8a849efc2a",
      "9a901c7408e344d7b457d1f4dd5845c3",
      "3aeb6d6b6a5543a19192a3e9840e288a",
      "f5246897496b4415aa4d4f6b2ad8d903",
      "93983cc8245f41dc88accc97ab0394a4",
      "642c1b53331049cc8e0990d880216523",
      "5d614b6a08fc437fb3b84acd7be31496",
      "f70a3d6c47d1490886129789a6437467",
      "d773d7a3bbfb4def9b624153c65feb5d",
      "edb6db3efb47435d947343d946c3921e",
      "461833ba6ebb4ba2a6d9f6b28f4453e2",
      "2e92bc27c66c4f829aec00c90e4f0841",
      "167ec2aee8f2464e9b2348c604cc6994",
      "afde6140990b4c04b4ff0d98ed6ccb1e",
      "e9f5e06f96894ff8aa3ffcc1278fd6b4"
     ]
    },
    "id": "b6078eaa-f0d1-41a4-96bb-d603453b3f8c",
    "outputId": "4da45937-f9da-4d7e-9ffd-c13dc052ef84"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c3eab5ecff4d57800f0a9e58f634f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5978 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9dd70aabc14174b4e5638912c45fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1495 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_train = ds['train'].map(formatting_prompts_func, batched = True)\n",
    "dataset_val = ds['val'].map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13cbdd2c-ae18-4191-98c3-6b1f3a1fe12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'response': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72',\n",
       " 'text': '<|im_start|>user\\n### Instruction:\\nCalculate only the first step for the following Math Word Problem\\n\\n### Input:\\nNatalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<|im_end|>\\n<|im_start|>assistant\\n### First Step:\\nNatalia sold 48/2 = <<48/2=24>>24 clips in May.<|im_end|>\\n<|im_start|>user\\n### Instruction:\\nContinue generating the entire answer from the next step\\n\\n<|im_end|>\\n<|im_start|>assistant\\n### Next steps:\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n## Final Answer: 72<|im_end|>\\n'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "Z3D-wHhQ5W6p",
   "metadata": {
    "id": "Z3D-wHhQ5W6p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4,\n",
       " 'question': 'Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?',\n",
       " 'response': 'Maila read 12 x 2 = <<12*2=24>>24 pages today.\\nSo she was able to read a total of 12 + 24 = <<12+24=36>>36 pages since yesterday.\\nThere are 120 - 36 = <<120-36=84>>84 pages left to be read.\\nSince she wants to read half of the remaining pages tomorrow, then she should read 84/2 = <<84/2=42>>42 pages.\\n#### 42'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['val'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108890cc-12c9-43e6-9392-e2b6868b61d2",
   "metadata": {
    "id": "108890cc-12c9-43e6-9392-e2b6868b61d2"
   },
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f166797-7f9d-4d86-bd5e-7fa2d48de8a2",
   "metadata": {
    "id": "0f166797-7f9d-4d86-bd5e-7fa2d48de8a2"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a097d80c-021d-440e-ade6-96b9abb1f6d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a097d80c-021d-440e-ade6-96b9abb1f6d8",
    "outputId": "2a809112-471d-443c-a310-0f04eb9bd99b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 28 layers with 0 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = SEED,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf5038e6-474d-4de3-b56f-1fb58fec16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "839e353f-e3c4-4d6f-b45f-faef36b3eb51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "839e353f-e3c4-4d6f-b45f-faef36b3eb51",
    "outputId": "f52fa20d-eed6-4e64-f8bd-c23cf70fa945"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgoyal\u001b[0m (\u001b[33meth-piyushi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /cluster/home/pgoyal/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=gemma-sft-lora-socratic\n"
     ]
    }
   ],
   "source": [
    "## wandb variables\n",
    "wandb.login(relogin=False, key='02f6b5d0ce8ce8ee2b69844245a2b3aae6af9582')\n",
    "%env WANDB_PROJECT=gemma-sft-lora-socratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b99d2d49-ac11-4eed-a1c8-183c979360ee",
   "metadata": {
    "id": "b99d2d49-ac11-4eed-a1c8-183c979360ee"
   },
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    # eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    save_total_limit = 1,\n",
    "    # metric_for_best_model = \"accuracy\",\n",
    "    logging_steps = 100,\n",
    "    # save_steps = 50, #unused because of epochs\n",
    "    per_device_train_batch_size = 4, #1 for gemma2_9b\n",
    "    # per_device_eval_batch_size = 2,\n",
    "    gradient_accumulation_steps = 2, #8 for gemma2_9b\n",
    "    warmup_steps = 100,\n",
    "    num_train_epochs = 4,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = SEED,\n",
    "    output_dir = OUTPUT_PATH,\n",
    "    # report_to = \"wandb\",\n",
    "    load_best_model_at_end=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58880c66-f73d-4bb9-b4d6-37ddd8becaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset_train,\n",
    "    # eval_dataset = dataset_val,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    # compute_metrics = compute_accuracy,\n",
    "    # dataset_num_proc = 2,\n",
    "    packing = True, \n",
    "    args = train_args\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "426cdf50-b391-41b0-af95-5a6e3bbe0625",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "id": "426cdf50-b391-41b0-af95-5a6e3bbe0625",
    "outputId": "b4c5e19f-f8b6-41f7-de99-1f1f1b1200c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cluster/home/pgoyal/main/test/COT/wandb/run-20240920_004549-aj9ifez3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eth-piyushi/gemma-sft-lora-socratic/runs/aj9ifez3' target=\"_blank\">clear-haze-68</a></strong> to <a href='https://wandb.ai/eth-piyushi/gemma-sft-lora-socratic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eth-piyushi/gemma-sft-lora-socratic' target=\"_blank\">https://wandb.ai/eth-piyushi/gemma-sft-lora-socratic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eth-piyushi/gemma-sft-lora-socratic/runs/aj9ifez3' target=\"_blank\">https://wandb.ai/eth-piyushi/gemma-sft-lora-socratic/runs/aj9ifez3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/eth-piyushi/gemma-sft-lora-socratic/runs/aj9ifez3?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x145a2cf5a790>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(settings=wandb.Settings(start_method='fork'), project='gemma-sft-lora-socratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40e6e9c7-d3b3-4990-866b-b3f4d001426c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40e6e9c7-d3b3-4990-866b-b3f4d001426c",
    "outputId": "10f773a6-ec17-408f-c8b6-bd2345ed7bd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.65 GB.\n",
      "14.805 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a344968-1759-4ad7-9e82-ad68a6ae582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e9b13f6-4ec1-47b4-822c-da6fc2f5c671",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "7e9b13f6-4ec1-47b4-822c-da6fc2f5c671",
    "outputId": "fc45ffd6-f808-46e0-ae8d-8e223a255238",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 1,417 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 8 | Total steps = 708\n",
      " \"-____-\"     Number of trainable parameters = 40,370,176\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='708' max='708' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [708/708 35:38, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.081100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad905d-58cc-4dd9-b140-8762c7ab072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b8cec5-4e52-4a0b-8848-5103e594d361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing checkpoint: /cluster/project/sachan/piyushi/final_predictions_COT/qwen_7/checkpoint-708\n",
      "==((====))==  Unsloth 2024.8: Fast Qwen2 patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.65 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ceac7848e548fbbb8b59191a7e4f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 28 layers with 0 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 326.62 out of 501.32 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 4/28 [00:00<00:00, 38.27it/s]We will save to Disk and not RAM now.\n",
      "100%|██████████| 28/28 [00:18<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n",
      "Merged model saved at: /cluster/project/sachan/piyushi/merged_models_COT/qwen_7/checkpoint-708\n",
      "All checkpoints processed and merged models saved.\n"
     ]
    }
   ],
   "source": [
    "# Loop over each checkpoint\n",
    "for checkpoint in sorted(os.listdir(OUTPUT_PATH)):\n",
    "    if checkpoint.startswith(\"checkpoint\"):\n",
    "        checkpoint_path = os.path.join(OUTPUT_PATH, checkpoint)\n",
    "        \n",
    "        if os.path.isdir(checkpoint_path):\n",
    "            print(f\"Processing checkpoint: {checkpoint_path}\")\n",
    "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "                model_name = checkpoint_path,\n",
    "                max_seq_length = MAX_SEQ_LENGTH,\n",
    "                dtype = DTYPE,\n",
    "                load_in_4bit = LOAD_IN_4BIT\n",
    "            )\n",
    "            # Merge the LoRA weights with the full model and save it\n",
    "            merged_output_dir = os.path.join(merged_dir, checkpoint)\n",
    "            model.save_pretrained_merged(merged_output_dir, tokenizer)\n",
    "            \n",
    "            print(f\"Merged model saved at: {merged_output_dir}\")\n",
    "            del model\n",
    "            del tokenizer  \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(\"All checkpoints processed and merged models saved.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sem_project",
   "language": "python",
   "name": "sem_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
