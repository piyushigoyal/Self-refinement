{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75b4201-65af-42ec-bebb-bc741406bf61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d75b4201-65af-42ec-bebb-bc741406bf61",
    "outputId": "9dc81871-d59f-4f4b-8f42-dfc0ea120431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 20 00:00:47 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:E1:00.0 Off |                  Off |\n",
      "| 30%   34C    P3              51W / 450W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc967f6-5907-48d6-8fd7-76ae42cc0e12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3326731-1cc8-4596-9049-3a7a4d9b3c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91bc8954-6537-4d1f-b3cd-0f29f91aaa9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip3 install \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "j7ch3dDfYgsj",
   "metadata": {
    "id": "j7ch3dDfYgsj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datasets\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import re\n",
    "import gc \n",
    "import os\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e4654b2-b9aa-4c4e-bfdd-3242d16d7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "DTYPE = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "LOAD_IN_4BIT = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "DATA_PATH = \"/cluster/home/pgoyal/main/test/Socratic/socratic_train.csv\"\n",
    "# MODEL_NAME = \"unsloth/gemma-2b-it\"\n",
    "# OUTPUT_PATH = \"/cluster/project/sachan/piyushi/final_predictions/gemma_2b\"\n",
    "# merged_dir = \"/cluster/project/sachan/piyushi/merged_models/gemma_2b\"\n",
    "MODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct\"\n",
    "OUTPUT_PATH = \"/cluster/project/sachan/piyushi/final_predictions/qwen_7\"\n",
    "merged_dir = \"/cluster/project/sachan/piyushi/merged_models/qwen_7\"\n",
    "SEED = 42\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f56ec6-99a7-42f0-832a-a97b27413cc2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382,
     "referenced_widgets": [
      "859111dfc6484a93b682427fe49c21f6",
      "84f60cec9dac4ea6bd2b13470ec4b097",
      "0351a104df874edf885ea696a63cdcea",
      "bacf7efd31fe495fbf14009a9a36c6d9",
      "3f2c28540c5d4ff7bfca8d53cf33599a",
      "fe9d546e76d84d3f977ecbf14d1cbe31",
      "0f8fb3ec0d9d4edc93669913dbc75c54",
      "c8a3971adbe3442c8640af9684a2509d",
      "6eeb4359964a45c98fdeb8dd954d6342",
      "b60f1a94d72f402eb3f0b5dfead1532a",
      "e8bdfb6d1b3a429fa83655180ba26101",
      "1214859312a74a4d952c8aa66b51b502",
      "2973dfba4b514cf98d8be3d3835f1d58",
      "060421c7756243ea8c44aad8b4dd8a8d",
      "988e6bc94d574b63b936d8dbbd0318c4",
      "3ffc2b15d085400bb70cf2e9dd2d3138",
      "202478512209441da0272d919c94a3c6",
      "2197b3204ac14659855c233b6316ee4d",
      "0328f7cd4dd04317aac32175e82920dd",
      "8ccd0348be8e47e996a2967916fc1650",
      "8945fd3df7d6403ea564ff90c531ff5f",
      "dc3127b9b6074788beec6f8a73fd20ac",
      "eba551f4cef645e4a5c8cdf6607efaf9",
      "01101eff78de4cb99c85617a796d133d",
      "3f90dd77826c49a8991c06177d4d66df",
      "b33ce1ebd82341a08ee25d7b78a5e531",
      "48aec7b783b3433d87de9354cd00e9df",
      "018405878b844f9ca12262b4fa22718a",
      "6c5e9ec3149f4082b7d3ddefa6b54538",
      "21374b31b5d04e909b139ad521442bb3",
      "ef59e1e5621f4df89b525658af407ac9",
      "dabab48f1de44f1ebb065f0cccd1f906",
      "0c4489c51c904bd888d31e2c7ef1b9ff",
      "fc8596d3cca248ec857faee5ae4c971d",
      "5b4b2a7035834bcda88c96f1498fbaf2",
      "847b226bed834b75b337a17ba40356dd",
      "2ee292c69c8445038eaadee2062618f6",
      "99678ccd77ba401fa3fd76bea9ba115e",
      "77fe7f75b37744d6a29de42b5bc2a27d",
      "07fd029518c345539da2c34f4e9c2e5a",
      "f2f61117276b4b619e1bbd77ccb6e8f0",
      "627a11ddc1074bf6b33e3c0d40a1b411",
      "77831f59dc224396a0b936491c0a5bd1",
      "8c411d40c82f412683fd888f827cd65b",
      "f9560828026d4dcc925825b047a775f0",
      "35f63e42ad154969a55e9a7dafb5131a",
      "3a3863b1d33641d6b2a7e4acb3f93e83",
      "15248b68ac7442c5a842ee6f4c7c2490",
      "3bf424767ceb4eaeaac47fd30c8c648c",
      "f66534d7240846bca4dcd12695f609d2",
      "1c9c8ea3c64f479eb98e49fe7bed0295",
      "bd681496164a40d18762a93901460cef",
      "91387aeab12f4dc6a7089f8338b65973",
      "5fb6b53d367e497ab781e27423f99ba6",
      "a67f7dd2ecf546fc90c84a75768436a0",
      "0b04f611162b4785a7736cda181bc002",
      "a9105b486b8d496586f7d7fc7b64e0b1",
      "cb1d26f2297e4848b1c4728fb8edeba6",
      "0817ec35e7cd46bc98111742c684023c",
      "b4e66b975e4a40e9aadfaa8830955a56",
      "684835aef4ca4f54bd4b311ef16e84c7",
      "a41ccb7d8a3440ad8b71ab63b4a74270",
      "f5cc63164ed5451f9f7a9fdd2de576ef",
      "c00d9ac1cda54f33b3a87b4debc50cb6",
      "31ce58be2e73465191c069852f3d6b3f",
      "c6712af364e24893acaa8921edf434f7",
      "84285fc67dd642bea2ddb25fe29cde80",
      "72673d88c5e3474d83adf64626bf0ae8",
      "0e9b27164e1445c6bf2d098531a7b537",
      "6e410518d60c40ceacae77e741f4da2e",
      "eb2c8f251df840c5ba110c2dbc4ebb8a",
      "fb3eff46f77f44c8bb90fbb921f1c09e",
      "f3e45d320a654facaaeca1b640e7be68",
      "a7a750c867574d0a98e492f7a4999026",
      "30e61f4b084f46d4bd642d54106f1dda",
      "2227ec266d644df4b5cd60163d196745",
      "abd831de9f434ba5a73167b44d2dbe4c"
     ]
    },
    "id": "76f56ec6-99a7-42f0-832a-a97b27413cc2",
    "outputId": "c0b3fee5-d4d8-4ba3-9849-db809417c609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Qwen2 patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.65 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54369af43f564b529718ec648425dacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350305cef9654dc7961fe1ea8749d855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45b48ee72ff4eefa2facefdec0a5d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0516a457af47bcbde9a63087e9c303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae62b6a60b0341b3b07c487f187d48bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f6eeaf5496488284d5488ef1fe6340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d00a45203e84255b096a6e677dd37c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bef5b7bc1f44a99e0be10ba565b15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8719d6e5fa48578d2b43f42fc0bdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1b3401d8a246f2bc5b48e7363800e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db0cc1557524ef8ad379149891f1951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9740db99e7460da946a216058f7893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d51fb9cb829478a8e723d761d777a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fb53c34c0d4d28bd8b4ec60a163065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_NAME,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = DTYPE,\n",
    "    load_in_4bit = LOAD_IN_4BIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6175cb5e-e1c3-4d22-bdbc-335007e49ef9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6175cb5e-e1c3-4d22-bdbc-335007e49ef9",
    "outputId": "96512995-ed4e-499a-cd49-b8bda1a3d810"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. No common IDs between train and validation.\\n2. See all IDs first [(id0-sample0, id1-sample0, id2-sample0,..., id1-sample1, id2-sample1, ...)]\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. No common IDs between train and validation.\n",
    "2. See all IDs first [(id0-sample0, id1-sample0, id2-sample0,..., id1-sample1, id2-sample1, ...)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9db0176a-8b18-4d36-9e51-4986ac689947",
   "metadata": {
    "id": "9db0176a-8b18-4d36-9e51-4986ac689947"
   },
   "outputs": [],
   "source": [
    "# Function to arrange dataframe such that the first n rows correspond to first n responses, and so on. \n",
    "# The idea is that the model sees all IDs first, instead of all responses from an ID \n",
    "def arrange_df(df, id_col='id', ques_col='question',response_col='response'):\n",
    "    df = df.sort_values(by=[id_col])\n",
    "    \n",
    "    # Group dataframe by 'id' column\n",
    "    grouped = df.groupby(id_col)\n",
    "    \n",
    "    # Initialize an empty list to store the arranged data\n",
    "    arranged_data = []\n",
    "    \n",
    "    # Get unique IDs and the maximum number of responses for any ID\n",
    "    unique_ids = df[id_col].unique()\n",
    "    max_responses = grouped.size().max()\n",
    "    \n",
    "    # Iterate over the number of responses (max_responses)\n",
    "    for i in range(max_responses):\n",
    "        # Iterate over unique IDs\n",
    "        for id_ in unique_ids:\n",
    "            # Get all responses and questions for the current ID\n",
    "            responses = grouped.get_group(id_)[response_col].tolist()\n",
    "            questions = grouped.get_group(id_)[ques_col].tolist()\n",
    "            # Append the ID, question, and response if available, else append None\n",
    "            if i < len(responses):\n",
    "                arranged_data.append({'id': id_, 'question': questions[i], 'response': responses[i]})\n",
    "            else:\n",
    "                arranged_data.append({'id': id_, 'question': None, 'response': None})\n",
    "    \n",
    "    # Create a new DataFrame from the arranged data\n",
    "    arranged_df = pd.DataFrame(arranged_data)\n",
    "    arranged_df = arranged_df.dropna(subset=['response'])\n",
    "    arranged_df.reset_index(drop=True, inplace=True)\n",
    "    print(arranged_df.head(10))\n",
    "    \n",
    "    assert(arranged_df.shape[0] == df.shape[0])\n",
    "    return arranged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc5N_JDvnMGO",
   "metadata": {
    "id": "dc5N_JDvnMGO"
   },
   "outputs": [],
   "source": [
    "def split_data(df, ratio=0.8):\n",
    "    \"\"\"\n",
    "    Splits the data into train and validation sets ensuring no common IDs among them.\n",
    "    \"\"\"\n",
    "    ids = list(set(df['id'].tolist()))\n",
    "    random.shuffle(ids)\n",
    "\n",
    "    ntrain = int(ratio*len(ids))\n",
    "    train_ids = ids[:ntrain]\n",
    "    val_ids = ids[ntrain:]\n",
    "\n",
    "    df_train = df[df['id'].isin(train_ids)].copy()\n",
    "    df_val = df[df['id'].isin(val_ids)].copy()\n",
    "\n",
    "    print(\"Train shape: \", df_train.shape)\n",
    "    print(\"Val shape: \", df_val.shape)\n",
    "    print(\"Data distribution: Train: {:.2f}, Val: {:.2f}\".format(df_train.shape[0]/len(df), df_val.shape[0]/len(df)))\n",
    "\n",
    "    return df_train, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "K8yYb5xQnP-M",
   "metadata": {
    "id": "K8yYb5xQnP-M"
   },
   "outputs": [],
   "source": [
    "def prepare_data(path=DATA_PATH, arrange_train=False, remove_train_duplicates=True, remove_val_duplicates=True, split_ratio=0.8):\n",
    "    df = pd.read_csv(path)\n",
    "    df.rename(columns={'answer':'response'}, inplace=True)\n",
    "    # Get Train and Val DFs\n",
    "    \n",
    "    df_train, df_val = split_data(df, split_ratio)\n",
    "    df_train = df_train.sort_values(by=['id'])\n",
    "    df_val = df_val.sort_values(by=['id'])\n",
    "    \n",
    "    if remove_train_duplicates:\n",
    "        print(\"Removing Train Duplicates\")\n",
    "        df_train = df_train.drop_duplicates(subset=['id'])\n",
    "        df_train.reset_index(drop=True, inplace=True) \n",
    "        print(df_train.shape)\n",
    "        print(df_train.head())\n",
    "    \n",
    "    # Arrange DF Train\n",
    "    if arrange_train:\n",
    "        df_train = arrange_df(df_train)\n",
    "    \n",
    "    # Remove Duplicates for Validation DF \n",
    "    if remove_val_duplicates:\n",
    "        print(\"Removing Val Duplicates\")\n",
    "        df_val = df_val.drop_duplicates(subset=['id'])\n",
    "        df_val.reset_index(drop=True, inplace=True)\n",
    "        print(df_val.shape)\n",
    "        print(df_val.head())\n",
    "        \n",
    "    # Convert to Dataset \n",
    "    dataset_train = datasets.Dataset.from_pandas(df_train[['id','question','response']].copy())\n",
    "    dataset_val = datasets.Dataset.from_pandas(df_val[['id','question','response']].copy())\n",
    "    \n",
    "    # Dataset Dict\n",
    "    ds = datasets.DatasetDict({\"train\":dataset_train, \"val\":dataset_val})\n",
    "    \n",
    "    print(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "-vYzmZ9SSDGO",
   "metadata": {
    "id": "-vYzmZ9SSDGO"
   },
   "outputs": [],
   "source": [
    "## Format Text\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Give the first subquestion and corresponding answer for the following Math Word Problem\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### First Subquestion:\n",
    "{}\n",
    "\n",
    "### First sub-answer:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"question\"]\n",
    "    outputs = examples[\"response\"]\n",
    "    texts = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        parts = output.split('\\n')[0].split('**')\n",
    "        subq, subans = parts[0], parts[1]\n",
    "        text = alpaca_prompt.format(input, subq.strip(), subans.strip()) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1915e4d-08fb-4d37-86ea-e2c321200b4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1915e4d-08fb-4d37-86ea-e2c321200b4f",
    "outputId": "977926ec-9bd8-417b-b9bb-4457b390229d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (5978, 3)\n",
      "Val shape:  (1495, 3)\n",
      "Data distribution: Train: 0.80, Val: 0.20\n",
      "Removing Train Duplicates\n",
      "(5978, 3)\n",
      "   id  \\\n",
      "0   1   \n",
      "1   2   \n",
      "2   3   \n",
      "3   6   \n",
      "4   7   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                question  \\\n",
      "0                                                                                                                            Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?   \n",
      "1                                                                                                                                                                      Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?   \n",
      "2                   Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?   \n",
      "3  Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?   \n",
      "4                                                              Albert is wondering how much pizza he can eat in one day. He buys 2 large pizzas and 2 small pizzas. A large pizza has 16 slices and a small pizza has 8 slices. If he eats it all, how many pieces does he eat that day?   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              response  \n",
      "0                                                                                                                                                                                                                                                                                                                                                                                          How many clips did Natalia sell in May? ** Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nHow many clips did Natalia sell altogether in April and May? ** Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72  \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                             How much does Weng earn per minute? ** Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\nHow much did Weng earn? ** Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\\n#### 10  \n",
      "2                                                                                                                                                                                                                                                              How much money does Betty have in the beginning? ** In the beginning, Betty has only 100 / 2 = $<<100/2=50>>50.\\nHow much money did Betty's grandparents give her? ** Betty's grandparents gave her 15 * 2 = $<<15*2=30>>30.\\nHow much more money does Betty need to buy the wallet? ** This means, Betty needs 100 - 50 - 30 - 15 = $<<100-50-30-15=5>>5 more.\\n#### 5  \n",
      "3  How many more purple flowers are there than yellow flowers? ** There are 80/100 * 10 = <<80/100*10=8>>8 more purple flowers than yellow flowers.\\nHow many purple flowers are there? ** So in Mark's garden, there are 10 + 8 = <<10+8=18>>18 purple flowers.\\nHow many flowers are there in total? ** Purple and yellow flowers sum up to 10 + 18 = <<10+18=28>>28 flowers.\\nHow many green flowers are there? ** That means in Mark's garden there are 25/100 * 28 = <<25/100*28=7>>7 green flowers.\\nHow many plants does Mark have in his garden? ** So in total Mark has 28 + 7 = <<28+7=35>>35 plants in his garden.\\n#### 35  \n",
      "4                                                                                                                                                                                                                                                                                                  How many slices does the largest pizza have? ** He eats 32 from the largest pizzas because 2 x 16 = <<2*16=32>>32\\nHow many slices does the small pizza have? ** He eats 16 from the small pizza because 2 x 8 = <<2*8=16>>16\\nHow many pieces does he eat that day? ** He eats 48 pieces because 32 + 16 = <<32+16=48>>48\\n#### 48  \n",
      "Removing Val Duplicates\n",
      "(1495, 3)\n",
      "   id  \\\n",
      "0   4   \n",
      "1   5   \n",
      "2  10   \n",
      "3  14   \n",
      "4  18   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                       question  \\\n",
      "0                                                                                                   Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?   \n",
      "1                                                                                                                                                                                                                       James writes a 3-page letter to 2 different friends twice a week.  How many pages does he write a year?   \n",
      "2                                                                                      Tina makes $18.00 an hour.  If she works more than 8 hours per shift, she is eligible for overtime, which is paid by your hourly wage + 1/2 your hourly wage.  If she works 10 hours every day for 5 days, how much money does she make?   \n",
      "3                                    Jasper will serve charcuterie at his dinner party. He buys 2 pounds of cheddar cheese for $10, a pound of cream cheese that cost half the price of the cheddar cheese, and a pack of cold cuts that cost twice the price of the cheddar cheese. How much does he spend on the ingredients?   \n",
      "4  In a truck, there are 26 pink hard hats, 15 green hard hats, and 24 yellow hard hats.  If Carl takes away 4 pink hard hats, and John takes away 6 pink hard hats and twice as many green hard hats as the number of pink hard hats that he removed, then calculate the total number of hard hats that remained in the truck.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           response  \n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             How many pages did Maila read today? ** Maila read 12 x 2 = <<12*2=24>>24 pages today.\\nHow many pages did Maila read since yesterday? ** So she was able to read a total of 12 + 24 = <<12+24=36>>36 pages since yesterday.\\nHow many pages are left to be read? ** There are 120 - 36 = <<120-36=84>>84 pages left to be read.\\nHow many pages should she read tomorrow? ** Since she wants to read half of the remaining pages tomorrow, then she should read 84/2 = <<84/2=42>>42 pages.\\n#### 42  \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              How many pages does he write each week? ** He writes each friend 3*2=<<3*2=6>>6 pages a week\\nHow many pages does he write every week? ** So he writes 6*2=<<6*2=12>>12 pages every week\\nHow many pages does he write a year? ** That means he writes 12*52=<<12*52=624>>624 pages a year\\n#### 624  \n",
      "2  How much does Tina make in an 8-hour shift? ** She works 8 hours a day for $18 per hour so she makes 8*18 = $<<8*18=144.00>>144.00 per 8-hour shift\\nHow many hours of overtime does Tina get? ** She works 10 hours a day and anything over 8 hours is eligible for overtime, so she gets 10-8 = <<10-8=2>>2 hours of overtime\\nHow much does Tina make in overtime? ** Overtime is calculated as time and a half so and she makes $18/hour so her overtime pay is 18*.5 = $<<18*.5=9.00>>9.00\\nHow much does Tina make in overtime? ** Her overtime pay is 18+9 = $<<18+9=27.00>>27.00\\nHow much does Tina make in a week? ** Her base pay is $144.00 per 8-hour shift and she works 5 days and makes 5 * $144 = $<<144*5=720.00>>720.00\\nHow much does Tina make in overtime? ** Her overtime pay is $27.00 per hour and she works 2 hours of overtime per day and makes 27*2 = $<<27*2=54.00>>54.00 in overtime pay\\nHow much does Tina make in overtime? ** 2 hours of overtime pay for 5 days means she makes 54*5 = $270.00\\nHow much does Tina make in a week? ** In 5 days her base pay is $720.00 and she makes $270.00 in overtime pay so she makes $720 + $270 = $<<720+270=990.00>>990.00\\n#### 990  \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 How much does a pound of cream cheese cost? ** A pound of cream cheese cost $10 / 2 = $<<10/2=5>>5.\\nHow much does a pack of cold cuts cost? ** A pack of cold cuts cost $10 x 2 = $<<10*2=20>>20.\\nHow much did Jasper spend on the ingredients? ** Jasper spent $10 + $5 + $20 = $<<10+5+20=35>>35 on the ingredients.\\n#### 35  \n",
      "4                                                                                                                                                     How many pink hard hats remained after Carl took away 4? ** If there were 26 pink hard hats and Carl took away 4 pink hard hats, the number of pink hard hats that remained is 26-4 = <<26-4=22>>22\\nHow many pink hard hats remained after John took away 6? ** John also took away 6 pink hard hats, leaving 22-6 = <<22-6=16>>16 pink hard hats in the truck.\\nHow many green hard hats did John take away? ** If John also took twice as many green hard hats as pink hard hats, he took 2*6 = <<6*2=12>>12 green hard hats.\\nHow many green hard hats remained after John took away 12? ** The total number of green hard hats that remained in the truck is 15-12 = <<15-12=3>>3\\nHow many hard hats remained in the truck after some are taken? ** In the truck, after some are taken, there were 3 green hard hats + 16 pink hard hats = <<3+16=19>>19 hard hats in the truck.\\nHow many hard hats remained in the truck? ** Altogether, 19 green and pink hard hats + 24 yellow hards hats = <<19+24=43>>43 hard hats remained in the truck\\n#### 43  \n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'response'],\n",
      "        num_rows: 5978\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['id', 'question', 'response'],\n",
      "        num_rows: 1495\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds = prepare_data(split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6078eaa-f0d1-41a4-96bb-d603453b3f8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "c69696c786e94ea8a8f6de2a88f185a0",
      "3b4630c1066a46eeadf5e40ffd9e6600",
      "ab10146d47fb43e3a1d491bfd87fe294",
      "5d57f55d6ce840d791159cc722a7af39",
      "efd1e04469f54a3cbdce102e6ab784d0",
      "50d629dedeba497a8f61bb502ecbd6a1",
      "665b1eaab4de400685c0f277c84ab47a",
      "c45fef4985e34e2a9151bd8a849efc2a",
      "9a901c7408e344d7b457d1f4dd5845c3",
      "3aeb6d6b6a5543a19192a3e9840e288a",
      "f5246897496b4415aa4d4f6b2ad8d903",
      "93983cc8245f41dc88accc97ab0394a4",
      "642c1b53331049cc8e0990d880216523",
      "5d614b6a08fc437fb3b84acd7be31496",
      "f70a3d6c47d1490886129789a6437467",
      "d773d7a3bbfb4def9b624153c65feb5d",
      "edb6db3efb47435d947343d946c3921e",
      "461833ba6ebb4ba2a6d9f6b28f4453e2",
      "2e92bc27c66c4f829aec00c90e4f0841",
      "167ec2aee8f2464e9b2348c604cc6994",
      "afde6140990b4c04b4ff0d98ed6ccb1e",
      "e9f5e06f96894ff8aa3ffcc1278fd6b4"
     ]
    },
    "id": "b6078eaa-f0d1-41a4-96bb-d603453b3f8c",
    "outputId": "4da45937-f9da-4d7e-9ffd-c13dc052ef84"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69883c6e69a4a9b8b1d481dc6118e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5978 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35827953dccb4ec7ab6c276945b72e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1495 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_train = ds['train'].map(formatting_prompts_func, batched = True)\n",
    "dataset_val = ds['val'].map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "207cad88-0c20-4ddf-9ab9-ce02a9f80400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'response': 'How many clips did Natalia sell in May? ** Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nHow many clips did Natalia sell altogether in April and May? ** Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72',\n",
       " 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive the first subquestion and corresponding answer for the following Math Word Problem\\n\\n### Input:\\nNatalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\\n\\n### First Subquestion:\\nHow many clips did Natalia sell in May?\\n\\n### First sub-answer:\\nNatalia sold 48/2 = <<48/2=24>>24 clips in May.\\n<|im_end|>'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "Z3D-wHhQ5W6p",
   "metadata": {
    "id": "Z3D-wHhQ5W6p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4,\n",
       " 'question': 'Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?',\n",
       " 'response': 'How many pages did Maila read today? ** Maila read 12 x 2 = <<12*2=24>>24 pages today.\\nHow many pages did Maila read since yesterday? ** So she was able to read a total of 12 + 24 = <<12+24=36>>36 pages since yesterday.\\nHow many pages are left to be read? ** There are 120 - 36 = <<120-36=84>>84 pages left to be read.\\nHow many pages should she read tomorrow? ** Since she wants to read half of the remaining pages tomorrow, then she should read 84/2 = <<84/2=42>>42 pages.\\n#### 42'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['val'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108890cc-12c9-43e6-9392-e2b6868b61d2",
   "metadata": {
    "id": "108890cc-12c9-43e6-9392-e2b6868b61d2"
   },
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f166797-7f9d-4d86-bd5e-7fa2d48de8a2",
   "metadata": {
    "id": "0f166797-7f9d-4d86-bd5e-7fa2d48de8a2"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a097d80c-021d-440e-ade6-96b9abb1f6d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a097d80c-021d-440e-ade6-96b9abb1f6d8",
    "outputId": "2a809112-471d-443c-a310-0f04eb9bd99b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 28 layers with 0 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = SEED,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf5038e6-474d-4de3-b56f-1fb58fec16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "839e353f-e3c4-4d6f-b45f-faef36b3eb51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "839e353f-e3c4-4d6f-b45f-faef36b3eb51",
    "outputId": "f52fa20d-eed6-4e64-f8bd-c23cf70fa945"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgoyal\u001b[0m (\u001b[33meth-piyushi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /cluster/home/pgoyal/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=gemma-sft-lora-socratic\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "## wandb variables\n",
    "wandb.login(relogin=False, key='02f6b5d0ce8ce8ee2b69844245a2b3aae6af9582')\n",
    "%env WANDB_PROJECT=gemma-sft-lora-socratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "144ad263-11e8-4af2-b66d-94a12149c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_logits_for_metrics(logits, labels):def preprocess_logits_for_metrics(logits, labels):\n",
    "#     if isinstance(logits, tuple):\n",
    "#         logits = logits[0]\n",
    "#     return logits.argmax(dim=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b99d2d49-ac11-4eed-a1c8-183c979360ee",
   "metadata": {
    "id": "b99d2d49-ac11-4eed-a1c8-183c979360ee"
   },
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    # eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    save_total_limit = 1,\n",
    "    # metric_for_best_model = \"accuracy\",\n",
    "    logging_steps = 50,\n",
    "    # save_steps = 100,\n",
    "    per_device_train_batch_size = 4,\n",
    "    # per_device_eval_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    warmup_steps = 100,\n",
    "    num_train_epochs = 4,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = SEED,\n",
    "    output_dir = OUTPUT_PATH,\n",
    "    # report_to = \"wandb\",\n",
    "    load_best_model_at_end=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58880c66-f73d-4bb9-b4d6-37ddd8becaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfc0c8870854fbb82c88d2a2e717e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset_train,\n",
    "    # eval_dataset = dataset_val,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    # compute_metrics = compute_accuracy,\n",
    "    # dataset_num_proc = 2,\n",
    "    packing = True, \n",
    "    args = train_args\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426cdf50-b391-41b0-af95-5a6e3bbe0625",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "id": "426cdf50-b391-41b0-af95-5a6e3bbe0625",
    "outputId": "b4c5e19f-f8b6-41f7-de99-1f1f1b1200c2"
   },
   "outputs": [],
   "source": [
    "wandb.init(settings=wandb.Settings(start_method='fork'), project='gemma-sft-lora-socratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40e6e9c7-d3b3-4990-866b-b3f4d001426c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40e6e9c7-d3b3-4990-866b-b3f4d001426c",
    "outputId": "10f773a6-ec17-408f-c8b6-bd2345ed7bd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.65 GB.\n",
      "14.805 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e9b13f6-4ec1-47b4-822c-da6fc2f5c671",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "7e9b13f6-4ec1-47b4-822c-da6fc2f5c671",
    "outputId": "fc45ffd6-f808-46e0-ae8d-8e223a255238",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 941 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 16 | Total steps = 236\n",
      " \"-____-\"     Number of trainable parameters = 40,370,176\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgoyal\u001b[0m (\u001b[33meth-piyushi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cluster/home/pgoyal/main/test/Socratic/wandb/run-20240919_233226-809xn1ez</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eth-piyushi/huggingface/runs/809xn1ez' target=\"_blank\">/cluster/project/sachan/piyushi/final_predictions/qwen_7</a></strong> to <a href='https://wandb.ai/eth-piyushi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eth-piyushi/huggingface' target=\"_blank\">https://wandb.ai/eth-piyushi/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eth-piyushi/huggingface/runs/809xn1ez' target=\"_blank\">https://wandb.ai/eth-piyushi/huggingface/runs/809xn1ez</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='236' max='236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [236/236 23:34, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.174300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.126700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad905d-58cc-4dd9-b140-8762c7ab072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b8cec5-4e52-4a0b-8848-5103e594d361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing checkpoint: /cluster/project/sachan/piyushi/final_predictions/qwen_7/checkpoint-236\n",
      "==((====))==  Unsloth 2024.8: Fast Qwen2 patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.65 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc4499ac3e44bf9ba0fb9143c5cb5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 28 layers with 0 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 328.14 out of 501.32 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 4/28 [00:00<00:00, 34.24it/s]We will save to Disk and not RAM now.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:19<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n",
      "Merged model saved at: /cluster/project/sachan/piyushi/merged_models/qwen_7/checkpoint-236\n",
      "All checkpoints processed and merged models saved.\n"
     ]
    }
   ],
   "source": [
    "# Loop over each checkpoint\n",
    "for checkpoint in sorted(os.listdir(OUTPUT_PATH)):\n",
    "    checkpoint_path = os.path.join(OUTPUT_PATH, checkpoint)\n",
    "    \n",
    "    if os.path.isdir(checkpoint_path):\n",
    "        print(f\"Processing checkpoint: {checkpoint_path}\")\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = checkpoint_path,\n",
    "            max_seq_length = MAX_SEQ_LENGTH,\n",
    "            dtype = DTYPE,\n",
    "            load_in_4bit = LOAD_IN_4BIT\n",
    "        )\n",
    "        # Merge the LoRA weights with the full model and save it\n",
    "        merged_output_dir = os.path.join(merged_dir, checkpoint)\n",
    "        model.save_pretrained_merged(merged_output_dir, tokenizer)\n",
    "        \n",
    "        print(f\"Merged model saved at: {merged_output_dir}\")\n",
    "        del model\n",
    "        del tokenizer  \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"All checkpoints processed and merged models saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9795d6-22e7-4420-962c-6720db7b9a89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60dd6406-8e64-47b8-a939-63e4249dafec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep 18 12:04:05 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:E1:00.0 Off |                  Off |\n",
      "| 30%   32C    P8              21W / 450W |      3MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dc2a18a-e4d6-49dc-b95e-167e9a140213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fbb6350-4e5c-4c6a-9ac6-3207deaa26da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_subquestion_answer(answer_text):\n",
    "    \"\"\"Extracts the final answer to the first subquestion after the equation.\"\"\"\n",
    "    # Split the answer into subquestions and answers\n",
    "    parts = answer_text.split('**')\n",
    "    answer_start=0\n",
    "    if len(parts) > 1:\n",
    "        first_answer = parts[1].split('##')[0].strip()\n",
    "        # if first_answer.lower().startswith(('define a variable', 'let\\'s assume', 'let')):\n",
    "        #     return 0.0\n",
    "        # Regex to find text after the equation, accounting for optional <<>>\n",
    "        match_patterns = (r'=.*?<<.*?>>', r\"=\\s+\")\n",
    "        if re.search(match_patterns[0], first_answer):\n",
    "            match = re.search(match_patterns[0], first_answer)\n",
    "            answer_start = match.end()\n",
    "        elif re.search(match_patterns[1], first_answer):\n",
    "            match = re.search(match_patterns[1], first_answer)\n",
    "            answer_start = match.end()\n",
    "\n",
    "        final_answer = first_answer[answer_start:].split('\\n')[0].strip()\n",
    "        # print(final_answer)\n",
    "        pattern = r'[$]?[-+]?\\d+(?:\\.\\d+)?(?:,\\d+)*[$]?'\n",
    "        matches = re.findall(pattern, final_answer)\n",
    "        if  matches != []:\n",
    "            ans = (float(matches[-1].replace(\",\", \"\").replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"$\", \"\").replace(\"x\", \"\")))\n",
    "            # print(ans)\n",
    "            return ans\n",
    "\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a41a491-0e0f-41e5-a64d-bddd84cc8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_labels(pred):\n",
    "    # Initialize variables for the prediction components\n",
    "    predicted_answer = None\n",
    "    \n",
    "    # Define markers to locate sections in the prediction\n",
    "    subquestion_marker = \"### First Subquestion:\\n\"\n",
    "    subanswer_marker = \"### First Sub-answer:\\n\"\n",
    "\n",
    "    # Find the start of the first subquestion and subanswer\n",
    "    subquestion_start = pred.find(subquestion_marker) + len(subquestion_marker)\n",
    "    subanswer_start = pred.find(subanswer_marker)\n",
    "\n",
    "    # Extract the subquestion (though this is not used in the return value)\n",
    "    subquestion = pred[subquestion_start:subanswer_start].strip()\n",
    "\n",
    "    # Extract and process the first sub-answer\n",
    "    subanswer_start += len(subanswer_marker)\n",
    "    entire_ans = pred[subanswer_start:].strip()\n",
    "\n",
    "    # Use regex to find the numeric answer in the sub-answer\n",
    "    pattern = r'[$]?[-+]?\\d+(?:\\.\\d+)?(?:,\\d+)*[$]?'\n",
    "    matches = re.findall(pattern, entire_ans)\n",
    "    \n",
    "    if matches != []:\n",
    "        predicted_answer = float(matches[-1].replace(\",\", \"\").replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"$\", \"\").replace(\"x\", \"\"))\n",
    "    \n",
    "    return predicted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "790f0f38-48f5-4abd-b680-5f79ab07cd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>correct_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>James writes a 3-page letter to 2 different friends twice a week.  How many pages does he write a year?</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tina makes $18.00 an hour.  If she works more than 8 hours per shift, she is eligible for overtime, which is paid by your hourly wage + 1/2 your hourly wage.  If she works 10 hours every day for 5 days, how much money does she make?</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jasper will serve charcuterie at his dinner party. He buys 2 pounds of cheddar cheese for $10, a pound of cream cheese that cost half the price of the cheddar cheese, and a pack of cold cuts that cost twice the price of the cheddar cheese. How much does he spend on the ingredients?</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In a truck, there are 26 pink hard hats, 15 green hard hats, and 24 yellow hard hats.  If Carl takes away 4 pink hard hats, and John takes away 6 pink hard hats and twice as many green hard hats as the number of pink hard hats that he removed, then calculate the total number of hard hats that remained in the truck.</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                       question  \\\n",
       "0                                                                                                   Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?   \n",
       "1                                                                                                                                                                                                                       James writes a 3-page letter to 2 different friends twice a week.  How many pages does he write a year?   \n",
       "2                                                                                      Tina makes $18.00 an hour.  If she works more than 8 hours per shift, she is eligible for overtime, which is paid by your hourly wage + 1/2 your hourly wage.  If she works 10 hours every day for 5 days, how much money does she make?   \n",
       "3                                    Jasper will serve charcuterie at his dinner party. He buys 2 pounds of cheddar cheese for $10, a pound of cream cheese that cost half the price of the cheddar cheese, and a pack of cold cuts that cost twice the price of the cheddar cheese. How much does he spend on the ingredients?   \n",
       "4  In a truck, there are 26 pink hard hats, 15 green hard hats, and 24 yellow hard hats.  If Carl takes away 4 pink hard hats, and John takes away 6 pink hard hats and twice as many green hard hats as the number of pink hard hats that he removed, then calculate the total number of hard hats that remained in the truck.   \n",
       "\n",
       "   correct_answer  \n",
       "0            24.0  \n",
       "1             6.0  \n",
       "2             8.0  \n",
       "3             5.0  \n",
       "4            22.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = []\n",
    "true_val_answers = []\n",
    "\n",
    "for item in ds['val']:\n",
    "    questions.append(item['question'])\n",
    "    true_val_answers.append(extract_first_subquestion_answer(item['response']))\n",
    "\n",
    "df_val = pd.DataFrame({\n",
    "    'question': questions,\n",
    "    'correct_answer': true_val_answers\n",
    "})\n",
    "\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3d58f23-cca1-4f44-9885-ff4a9e7eb430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(df_predictions):\n",
    "    # Initialize accuracy dictionary \n",
    "    acc_dict = {'checkpoint':[], 'val_acc':[]}\n",
    "\n",
    "    # Get a list of columns with predictions \n",
    "    pred_cols = [col for col in list(df_predictions.columns) if col.startswith('predictions')]\n",
    "    \n",
    "    # Iterate over prediction columns \n",
    "    for pred_col in pred_cols:\n",
    "        df_predictions['num_answer_{x}'.format(x=pred_col)] = df_predictions[pred_col].apply(lambda x: pred_labels(x))\n",
    "        # print(df_predictions.head())\n",
    "        acc = np.round(100*(df_predictions['correct_answer'] == df_predictions['num_answer_{x}'.format(x=pred_col)]).sum()/df_predictions.shape[0], 2)\n",
    "        acc_dict[\"checkpoint\"].append(pred_col)\n",
    "        acc_dict['val_acc'].append(acc)\n",
    "\n",
    "    return pd.DataFrame(acc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8016af2f-41b3-4403-ad2a-6c52c1156ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "def predict_from_checkpoint_vllm(checkpoint_path, max_seq_len, questions):\n",
    "    llm = LLM(model=checkpoint_path, max_model_len=512)\n",
    "    sampling_params = SamplingParams(temperature=0, max_tokens=max_seq_len)\n",
    "    # sampling_params = SamplingParams(temperature=0, max_tokens=max_seq_len, stop=[\"### Instruction\", \"### Input\"]))\n",
    "    predictions = []\n",
    "    outputs = llm.generate(questions, sampling_params)\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        predictions.append(prompt + \"\\n\" + generated_text)\n",
    "    \n",
    "    assert(len(predictions)==len(questions))\n",
    "\n",
    "    # Free Memory \n",
    "    torch.cuda.empty_cache()\n",
    "    del llm\n",
    "    del sampling_params \n",
    "    gc.collect()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e7ed274-bbfd-4e6f-8964-0ffd9d34fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format Text\n",
    "alpaca_val_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Give the first subquestion and corresponding answer for the following Math Word Problem\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### First Subquestion:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "def format_question(q, prompt):\n",
    "    return prompt.format(q, \"\")\n",
    "\n",
    "def run(model_dir, max_seq_len):\n",
    "    # Load and preprocess val data\n",
    "    # dataset_val = load_preprocess_data(model_dir)\n",
    "    checkpoint_paths = [os.path.join(model_dir, x) for x in os.listdir(model_dir) if x.startswith(\"checkpoint\")]\n",
    "\n",
    "    # Format Validation Questions \n",
    "    formatted_questions = df_val[\"question\"].apply(lambda x: format_question(x, alpaca_val_prompt)).tolist()\n",
    "    \n",
    "    # Load checkpoints and predict in a loop \n",
    "    for checkpoint_path in checkpoint_paths:\n",
    "        predictions = predict_from_checkpoint_vllm(checkpoint_path, max_seq_len, formatted_questions)\n",
    "        chk = checkpoint_path.split(\"/\")[-1]\n",
    "        df_val['predictions_{x}'.format(x=chk)] = predictions\n",
    "        print(\"Predictions from checkpoint {chk} completed\".format(chk=chk))\n",
    "\n",
    "    # Save DF \n",
    "    # dataset_val.to_csv(os.path.join(model_dir, 'df-val-preds-all-checkpoints.csv'), index=False)\n",
    "\n",
    "    # Get accuracy\n",
    "    df_acc = get_accuracy(df_val)\n",
    "    print(df_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d138218-d0c6-47b7-8816-b2cbc3def357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-18 12:04:16 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-124', speculative_config=None, tokenizer='/cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-124', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-124, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 09-18 12:04:16 model_runner.py:720] Starting to load model /cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-124...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W918 12:04:16.338540924 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a0d40a292c486b86190ac67bd32cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-18 12:04:19 model_runner.py:732] Loading model weights took 16.0120 GB\n",
      "INFO 09-18 12:04:20 gpu_executor.py:102] # GPU blocks: 485, # CPU blocks: 585\n",
      "INFO 09-18 12:04:23 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-18 12:04:23 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-18 12:04:41 model_runner.py:1225] Graph capturing finished in 17 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                                              | 0/1495 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:04:42 scheduler.py:1099] Sequence group 61 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 275/1495 [00:13<00:52, 23.27it/s, est. speed input: 2310.89 toks/s, output: 857.87 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:04:55 scheduler.py:1099] Sequence group 325 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 568/1495 [00:27<00:44, 20.93it/s, est. speed input: 2407.01 toks/s, output: 890.15 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:05:09 scheduler.py:1099] Sequence group 621 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 892/1495 [00:43<00:27, 21.87it/s, est. speed input: 2455.35 toks/s, output: 905.33 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:05:24 scheduler.py:1099] Sequence group 943 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1225/1495 [00:58<00:13, 19.30it/s, est. speed input: 2466.82 toks/s, output: 911.26 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:05:40 scheduler.py:1099] Sequence group 1278 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1495/1495 [01:10<00:00, 21.21it/s, est. speed input: 2516.32 toks/s, output: 930.18 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions from checkpoint checkpoint-124 completed\n",
      "INFO 09-18 12:05:52 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-186', speculative_config=None, tokenizer='/cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-186', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-186, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 09-18 12:05:53 model_runner.py:720] Starting to load model /cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-186...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03edafe600754eceb08bf3ba4280e7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-18 12:08:25 model_runner.py:732] Loading model weights took 16.0120 GB\n",
      "INFO 09-18 12:08:25 gpu_executor.py:102] # GPU blocks: 499, # CPU blocks: 585\n",
      "INFO 09-18 12:08:25 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-18 12:08:25 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-18 12:08:42 model_runner.py:1225] Graph capturing finished in 17 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                                              | 0/1495 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:08:43 scheduler.py:1099] Sequence group 63 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 313/1495 [00:15<00:53, 21.90it/s, est. speed input: 2424.48 toks/s, output: 873.58 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:08:58 scheduler.py:1099] Sequence group 370 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 664/1495 [00:31<00:35, 23.19it/s, est. speed input: 2526.59 toks/s, output: 916.72 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:09:14 scheduler.py:1099] Sequence group 723 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 1015/1495 [00:47<00:23, 20.66it/s, est. speed input: 2544.41 toks/s, output: 917.91 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:09:30 scheduler.py:1099] Sequence group 1069 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1415/1495 [01:05<00:03, 20.05it/s, est. speed input: 2554.74 toks/s, output: 929.63 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:09:48 scheduler.py:1099] Sequence group 1470 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1495/1495 [01:08<00:00, 21.84it/s, est. speed input: 2590.16 toks/s, output: 942.85 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions from checkpoint checkpoint-186 completed\n",
      "INFO 09-18 12:09:51 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-248', speculative_config=None, tokenizer='/cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-248', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-248, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 09-18 12:09:53 model_runner.py:720] Starting to load model /cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-248...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b90c9c9e4f45378271730531c5f783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-18 12:12:09 model_runner.py:732] Loading model weights took 16.0120 GB\n",
      "INFO 09-18 12:12:10 gpu_executor.py:102] # GPU blocks: 499, # CPU blocks: 585\n",
      "INFO 09-18 12:12:10 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-18 12:12:10 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-18 12:12:27 model_runner.py:1225] Graph capturing finished in 17 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                                              | 0/1495 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:12:28 scheduler.py:1099] Sequence group 63 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 227/1495 [00:11<00:50, 25.35it/s, est. speed input: 2229.36 toks/s, output: 876.55 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:12:40 scheduler.py:1099] Sequence group 279 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 581/1495 [00:28<00:37, 24.37it/s, est. speed input: 2368.85 toks/s, output: 930.41 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:12:56 scheduler.py:1099] Sequence group 632 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 880/1495 [00:43<00:28, 21.21it/s, est. speed input: 2398.99 toks/s, output: 942.94 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:13:11 scheduler.py:1099] Sequence group 931 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 1162/1495 [00:57<00:16, 20.13it/s, est. speed input: 2412.54 toks/s, output: 946.60 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:13:25 scheduler.py:1099] Sequence group 1217 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1495/1495 [01:12<00:00, 20.71it/s, est. speed input: 2457.19 toks/s, output: 961.76 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions from checkpoint checkpoint-248 completed\n",
      "INFO 09-18 12:13:40 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-62', speculative_config=None, tokenizer='/cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-62', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-62, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 09-18 12:13:41 model_runner.py:720] Starting to load model /cluster/project/sachan/piyushi/merged_models/gemma2_9b/checkpoint-62...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945a4426515a421f91c56e59a1838319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-18 12:15:59 model_runner.py:732] Loading model weights took 16.0120 GB\n",
      "INFO 09-18 12:15:59 gpu_executor.py:102] # GPU blocks: 499, # CPU blocks: 585\n",
      "INFO 09-18 12:15:59 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-18 12:15:59 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-18 12:16:17 model_runner.py:1225] Graph capturing finished in 18 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                                              | 0/1495 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:16:19 scheduler.py:1099] Sequence group 63 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 264/1495 [00:12<00:49, 24.68it/s, est. speed input: 2454.31 toks/s, output: 846.40 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:16:31 scheduler.py:1099] Sequence group 319 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 579/1495 [00:26<00:35, 26.09it/s, est. speed input: 2585.63 toks/s, output: 896.95 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:16:44 scheduler.py:1099] Sequence group 633 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 981/1495 [00:44<00:21, 24.18it/s, est. speed input: 2641.81 toks/s, output: 900.61 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:17:02 scheduler.py:1099] Sequence group 1034 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1333/1495 [00:59<00:07, 21.08it/s, est. speed input: 2642.40 toks/s, output: 912.49 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-18 12:17:18 scheduler.py:1099] Sequence group 1384 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1495/1495 [01:06<00:00, 22.56it/s, est. speed input: 2675.65 toks/s, output: 926.66 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions from checkpoint checkpoint-62 completed\n",
      "                   checkpoint  val_acc\n",
      "0  predictions_checkpoint-124    44.48\n",
      "1  predictions_checkpoint-186    54.18\n",
      "2  predictions_checkpoint-248    54.45\n",
      "3   predictions_checkpoint-62    43.48\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description=\"Validation\")\n",
    "#     parser.add_argument(\"--max_seq_len\", type=int, default=512, help=\"Max model sequence length\")\n",
    "#     args = parser.parse_args()\n",
    "#     run(merged_dir,  args.max_seq_len)\n",
    "\n",
    "run(merged_dir, MAX_SEQ_LENGTH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sem_project",
   "language": "python",
   "name": "sem_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
